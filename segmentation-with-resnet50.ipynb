{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **SETUP**\n\n---","metadata":{"id":"seJCTy8VZALZ"}},{"cell_type":"code","source":"%pip install -q pytorch-lightning torchmetrics wandb\n\nimport os\nimport random\nfrom typing import List, Tuple\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pytorch_lightning as pl\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.metrics import accuracy_score, confusion_matrix, jaccard_score\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchmetrics import Accuracy, ConfusionMatrix, JaccardIndex\nfrom torchvision import transforms\nfrom torchvision.transforms import transforms\nfrom tqdm import tqdm\n\n\n# Log into Wandb\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb-key\")\n!wandb login $wandb_key","metadata":{"id":"9IcDeZ1oXeVI","execution":{"iopub.status.busy":"2023-04-27T18:18:22.760755Z","iopub.execute_input":"2023-04-27T18:18:22.761473Z","iopub.status.idle":"2023-04-27T18:18:51.593574Z","shell.execute_reply.started":"2023-04-27T18:18:22.761438Z","shell.execute_reply":"2023-04-27T18:18:51.592082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DATASET**\n\n---","metadata":{"id":"Bc3V2jXsZGRE"}},{"cell_type":"code","source":"class AI4MARSDataset(Dataset):\n    def __init__(self, images_path: str, masks_path: str, dataset_size: int = 500):\n        self.images_path = images_path\n        self.masks_path = masks_path\n        self.dataset_size = dataset_size\n        \n        images = set(os.listdir(images_path))\n        self.masks = [mask for mask in os.listdir(masks_path) if mask[:-4] + \".JPG\" in images][:dataset_size]\n\n    def __len__(self):\n        return len(self.masks)\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        mask_name = self.masks[idx]\n\n        image_path = os.path.join(self.images_path, mask_name[:-4] + \".JPG\")\n        image = cv2.imread(image_path)\n        image = cv2.resize(image, (224, 224))\n        image = np.asarray(image, dtype=np.float32) / 255.0\n        image = np.transpose(image, (2, 0, 1))  # Change the order of dimensions to (C, H, W)\n        image = torch.from_numpy(image)\n\n        mask_path = os.path.join(self.masks_path, mask_name)\n        mask = cv2.imread(mask_path, 0)\n        mask = cv2.resize(mask, (224, 224), interpolation=cv2.INTER_NEAREST)\n        mask = np.array(mask, dtype=np.uint8)\n        mask[mask == 255] = 4\n        mask = torch.from_numpy(mask)\n        mask = mask.long()\n\n        return image, mask\n\nclass AI4MARSDataModule(pl.LightningDataModule):\n    def __init__(self, images_path: str, masks_path: str, dataset_size: int = 5000, batch_size: int = 32, num_workers: int = 4):\n        super().__init__()\n        self.images_path = images_path\n        self.masks_path = masks_path\n        self.dataset_size = dataset_size\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def setup(self, stage=None):\n        self.dataset = AI4MARSDataset(self.images_path, self.masks_path, self.dataset_size)\n\n    def train_dataloader(self):\n        return DataLoader(self.dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True, pin_memory=True)","metadata":{"id":"QaDnO_LkXlKr","execution":{"iopub.status.busy":"2023-04-27T18:18:51.597736Z","iopub.execute_input":"2023-04-27T18:18:51.598103Z","iopub.status.idle":"2023-04-27T18:18:51.616074Z","shell.execute_reply.started":"2023-04-27T18:18:51.598067Z","shell.execute_reply":"2023-04-27T18:18:51.613849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL**\n\n---","metadata":{"id":"8f90jqwJZIbF"}},{"cell_type":"code","source":"class ImageSegmentationModel(pl.LightningModule):\n    def __init__(self, num_classes: int = 5, learning_rate: float = 1e-4):\n        super().__init__()\n\n        self.save_hyperparameters()\n        self.learning_rate = learning_rate\n        self.num_classes = num_classes\n\n        self.model_weights = torchvision.models.segmentation.FCN_ResNet50_Weights.DEFAULT\n        self.model = torchvision.models.segmentation.fcn_resnet50(weights=self.model_weights)\n        self.model.classifier[-1] = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1))\n\n        self.loss = nn.CrossEntropyLoss()\n        self.confusion_matrix = ConfusionMatrix(task='multiclass', num_classes=num_classes)\n        self.accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n        self.iou = JaccardIndex(task='multiclass', num_classes=num_classes)\n\n    def forward(self, x):\n        return self.model(x)['out']\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss(preds, y)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss(preds, y)\n        preds = torch.argmax(preds, dim=1)\n\n        # Log metrics\n        self.log('val_loss', loss, on_step=True, on_epoch=True)\n        self.log('val_acc', self.accuracy(preds, y), on_step=True, on_epoch=True)\n        self.log('val_iou', self.iou(preds, y), on_step=True, on_epoch=True)","metadata":{"id":"Nvc0X0mlXnpt","execution":{"iopub.status.busy":"2023-04-27T18:18:51.617474Z","iopub.execute_input":"2023-04-27T18:18:51.617906Z","iopub.status.idle":"2023-04-27T18:18:51.639874Z","shell.execute_reply.started":"2023-04-27T18:18:51.617873Z","shell.execute_reply":"2023-04-27T18:18:51.638666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAINING**\n\n---","metadata":{"id":"Cc4_-VJBZKlm"}},{"cell_type":"code","source":"def train_single_run(data_module: AI4MARSDataModule, epochs: int = 10):\n\n    # Split the dataset into training and validation sets\n    dataset_size = len(data_module.dataset)\n    train_size = int(dataset_size * 0.8)\n    val_size = dataset_size - train_size\n    train_dataset, val_dataset = random_split(data_module.dataset, [train_size, val_size])\n\n    # Set up data loaders\n    train_dataloader = DataLoader(train_dataset, batch_size=data_module.batch_size, shuffle=True, num_workers=data_module.num_workers)\n    val_dataloader = DataLoader(val_dataset, batch_size=data_module.batch_size, num_workers=data_module.num_workers)\n\n    # Initialize the Lightning model\n    model = ImageSegmentationModel()\n\n    # Set up Weights & Biases logger\n    wandb_logger = pl.loggers.WandbLogger(name=\"AI4MARS_single_run\", project=\"AI4MARS\")\n\n    # Set up checkpoint callback\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_loss\",\n        dirpath=\"checkpoints\",\n        filename=\"checkpoint-{epoch:02d}-{val_loss:.4f}\",\n        save_top_k=1,\n        mode=\"min\",\n    )\n\n    # Set up the Lightning trainer\n    trainer = pl.Trainer(logger=wandb_logger, max_epochs=epochs, accelerator='gpu', devices=-1, callbacks=[checkpoint_callback], log_every_n_steps=50)\n\n    # Train and validate the model\n    trainer.fit(model, train_dataloader, val_dataloader)\n    \n    # Save the model checkpoint after training\n    torch.save(model.state_dict(), \"trained_model_checkpoint.pth\")\n\n    # End current run\n    wandb.finish()\n    \n    # Return summary\n    return wandb_logger.experiment.summary","metadata":{"id":"hsXNSAU0xRUK","execution":{"iopub.status.busy":"2023-04-27T18:18:51.643497Z","iopub.execute_input":"2023-04-27T18:18:51.644068Z","iopub.status.idle":"2023-04-27T18:18:51.656115Z","shell.execute_reply.started":"2023-04-27T18:18:51.644027Z","shell.execute_reply":"2023-04-27T18:18:51.655000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EXECUTION**\n\n---","metadata":{"id":"pxAdvlp7ZUAo"}},{"cell_type":"code","source":"# Paths\nIMAGES_PATH = \"/kaggle/input/ai4mars-terrainaware-autonomous-driving-on-mars/ai4mars-dataset-merged-0.1/msl/images/edr\"\nMASK_PATH_TRAIN = \"/kaggle/input/ai4mars-terrainaware-autonomous-driving-on-mars/ai4mars-dataset-merged-0.1/msl/labels/train\"\nMASK_PATH_TEST = \"/kaggle/input/ai4mars-terrainaware-autonomous-driving-on-mars/ai4mars-dataset-merged-0.1/msl/labels/test/masked-gold-min3-100agree/\"\n\n# Hyperparameters\nDATASET_SIZE = 15000\nBATCH_SIZE = 32\nEPOCHS = 25","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:18:51.658001Z","iopub.execute_input":"2023-04-27T18:18:51.658448Z","iopub.status.idle":"2023-04-27T18:18:51.672284Z","shell.execute_reply.started":"2023-04-27T18:18:51.658408Z","shell.execute_reply":"2023-04-27T18:18:51.671152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\nrock_data = AI4MARSDataModule(IMAGES_PATH, MASK_PATH_TRAIN, dataset_size=DATASET_SIZE, batch_size=BATCH_SIZE, num_workers=2)\nrock_data.setup()\n\n# Train model\nsummary = train_single_run(rock_data, epochs=EPOCHS)","metadata":{"id":"SQpZ0Qr7MOeR","outputId":"522f84e6-b55e-4cd9-b4a6-e77354a535ec","execution":{"iopub.status.busy":"2023-04-27T18:18:51.675291Z","iopub.execute_input":"2023-04-27T18:18:51.676297Z","iopub.status.idle":"2023-04-27T18:19:57.259053Z","shell.execute_reply.started":"2023-04-27T18:18:51.676237Z","shell.execute_reply":"2023-04-27T18:19:57.257861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DEMONSTRATION**\n\n---","metadata":{}},{"cell_type":"code","source":"# Preprocess images\ndef preprocess_image(image_path: str, return_tensor: bool = False):\n    image = cv2.imread(image_path)\n    image = cv2.resize(image, (224, 224))\n    image_normalized = np.asarray(image, dtype=np.float32) / 255.0\n    \n    if return_tensor:\n        image_normalized = np.transpose(image_normalized, (2, 0, 1))\n        image_tensor = torch.from_numpy(image_normalized).unsqueeze(0)\n        return image, image_tensor\n    \n    return image\n\n# Preprocess masks\ndef preprocess_mask(mask_path: str):\n    mask = cv2.imread(mask_path, 0)\n    mask = cv2.resize(mask, (224, 224), interpolation=cv2.INTER_NEAREST)\n    mask[mask == 255] = 4\n    return mask\n\n# Find an image in the test set\ndef display_segmentation(index):\n    mask_files = [filename for filename in os.listdir(MASK_PATH_TEST) if filename.endswith(\"_merged.png\")]\n    test_image_name = mask_files[index][:-11] + \".JPG\"\n\n    # Load the test image\n    test_image_path = os.path.join(IMAGES_PATH, test_image_name)\n    test_image, test_image_tensor = preprocess_image(test_image_path, return_tensor=True)\n    test_image_tensor = test_image_tensor.to(device)\n\n    # Perform prediction\n    with torch.no_grad():\n        prediction = model(test_image_tensor)\n        predicted_mask = torch.argmax(prediction, dim=1).squeeze().cpu().numpy()\n\n    # Load the ground truth segmentation\n    ground_truth_mask_path = os.path.join(MASK_PATH_TEST, test_image_name[:-4] + \"_merged.png\")\n    ground_truth_mask = preprocess_mask(ground_truth_mask_path)\n\n    return test_image, ground_truth_mask, predicted_mask","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:19:57.261524Z","iopub.execute_input":"2023-04-27T18:19:57.262850Z","iopub.status.idle":"2023-04-27T18:19:57.275467Z","shell.execute_reply.started":"2023-04-27T18:19:57.262803Z","shell.execute_reply":"2023-04-27T18:19:57.274307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the saved model\nmodel = ImageSegmentationModel()\nmodel.load_state_dict(torch.load(\"/kaggle/working/trained_model_checkpoint.pth\"))\nmodel.eval()\nmodel.to(device)\n\n# Choose indices\nindices = [0, 50, 100]\nsegmentations = [display_segmentation(index) for index in indices]\n\n# Plot the images\nfig, axes = plt.subplots(len(indices), 3, figsize=(15, 15))\nfor i, (test_image, ground_truth_mask, predicted_mask) in enumerate(segmentations):\n    axes[i, 0].imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n    axes[i, 0].set_title(\"Original image\")\n    axes[i, 1].imshow(ground_truth_mask, cmap=\"inferno\")\n    axes[i, 1].set_title(\"Crowdsourced segmentation\")\n    axes[i, 2].imshow(predicted_mask, cmap=\"inferno\")\n    axes[i, 2].set_title(\"Predicted segmentation\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:19:57.277241Z","iopub.execute_input":"2023-04-27T18:19:57.277724Z","iopub.status.idle":"2023-04-27T18:20:01.091865Z","shell.execute_reply.started":"2023-04-27T18:19:57.277666Z","shell.execute_reply":"2023-04-27T18:20:01.089365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EVALUATION**\n\n---","metadata":{}},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, images_path: str, masks_path: str):\n        self.images_path = images_path\n        self.masks_path = masks_path\n        self.mask_files = [mask for mask in os.listdir(masks_path) if mask.endswith(\"_merged.png\")]\n\n    def __len__(self):\n        return len(self.mask_files)\n\n    def __getitem__(self, idx):\n        mask_name = self.mask_files[idx]\n        image_name = mask_name[:-11] + \".JPG\"\n\n        image_path = os.path.join(self.images_path, image_name)\n        _, image_tensor = preprocess_image(image_path, return_tensor=True)\n\n        mask_path = os.path.join(self.masks_path, mask_name)\n        mask = preprocess_mask(mask_path)\n\n        # Remove the extra dimension\n        image_tensor = image_tensor.squeeze(0)\n\n        return image_tensor, mask","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:20:01.093003Z","iopub.execute_input":"2023-04-27T18:20:01.093483Z","iopub.status.idle":"2023-04-27T18:20:01.104820Z","shell.execute_reply.started":"2023-04-27T18:20:01.093429Z","shell.execute_reply":"2023-04-27T18:20:01.103706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_test_set(model, images_path, masks_path, batch_size=32, num_workers=2, device='cuda'):\n    \n    # Load dataloaders\n    test_dataset = TestDataset(images_path, masks_path)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n\n    # Initialize arrays for storing ground truth and predictions\n    ground_truths = []\n    predictions = []\n\n    for batch_images, batch_masks in tqdm(test_dataloader):\n        # Move the images to the GPU\n        batch_images = batch_images.to(device)\n\n        # Perform prediction\n        with torch.no_grad():\n            batch_prediction = model(batch_images)\n            batch_predicted_masks = torch.argmax(batch_prediction, dim=1).cpu().numpy()\n\n        # Flatten and append the ground truth and predictions\n        ground_truths.extend([mask.flatten() for mask in batch_masks])\n        predictions.extend([mask.flatten() for mask in batch_predicted_masks])\n\n    # Compute accuracy\n    accuracy = accuracy_score(np.concatenate(ground_truths), np.concatenate(predictions))\n\n    # Compute IoU (Jaccard score)\n    iou = jaccard_score(np.concatenate(ground_truths), np.concatenate(predictions), average=\"weighted\")\n\n    # Compute confusion matrix\n    cm = confusion_matrix(np.concatenate(ground_truths), np.concatenate(predictions))\n    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\n    return accuracy, iou, cm_normalized","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:20:01.109970Z","iopub.execute_input":"2023-04-27T18:20:01.111169Z","iopub.status.idle":"2023-04-27T18:20:01.124636Z","shell.execute_reply.started":"2023-04-27T18:20:01.111123Z","shell.execute_reply":"2023-04-27T18:20:01.123269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\naccuracy, iou, cm_normalized = evaluate_test_set(model, IMAGES_PATH, MASK_PATH_TEST, device=device)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"IoU: {iou:.4f}\")\n\n# Plot the normalized confusion matrix using Seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_normalized, annot=True, fmt=\".4f\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Normalized Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:20:01.138982Z","iopub.execute_input":"2023-04-27T18:20:01.139809Z","iopub.status.idle":"2023-04-27T18:20:16.919618Z","shell.execute_reply.started":"2023-04-27T18:20:01.139763Z","shell.execute_reply":"2023-04-27T18:20:16.918301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}